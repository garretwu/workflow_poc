{
  "phase": "pre",
  "scenario": "default",
  "metrics": {
    "latency_ms": {
      "p50": 140,
      "p95": 600,
      "p99": 1200
    },
    "tokens_per_sec": 8200,
    "queue_depth": 65,
    "gpu_util": 0.78,
    "gpu_mem_util": 0.90,
    "cpu_util": 0.62,
    "batch_size": 32,
    "max_num_batched_tokens": 8192
  },
  "config": {
    "model": "llama-2-7b",
    "max_model_len": 4096,
    "max_num_batched_tokens": 8192,
    "max_num_seqs": 64,
    "gpu_memory_utilization": 0.90,
    "enable_paged_kv_cache": false,
    "tokenizer_pool_size": 0,
    "async_output_processing": false
  },
  "logs": [
    "WARN scheduler queue depth exceeded 60",
    "WARN kv cache eviction rate high",
    "INFO prefill stage slower than target",
    "INFO batching up to 8192 tokens"
  ]
}
