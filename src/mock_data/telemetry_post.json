{
  "phase": "post",
  "scenario": "default",
  "metrics": {
    "latency_ms": {
      "p50": 90,
      "p95": 240,
      "p99": 420
    },
    "tokens_per_sec": 11000,
    "queue_depth": 18,
    "gpu_util": 0.82,
    "gpu_mem_util": 0.82,
    "cpu_util": 0.55,
    "batch_size": 24,
    "max_num_batched_tokens": 4096
  },
  "config": {
    "model": "llama-2-7b",
    "max_model_len": 3072,
    "max_num_batched_tokens": 4096,
    "max_num_seqs": 48,
    "gpu_memory_utilization": 0.85,
    "enable_paged_kv_cache": true,
    "tokenizer_pool_size": 4,
    "async_output_processing": true
  },
  "logs": [
    "INFO queue depth stabilized",
    "INFO paged kv cache enabled",
    "INFO throughput recovered",
    "INFO batch size adjusted"
  ]
}
